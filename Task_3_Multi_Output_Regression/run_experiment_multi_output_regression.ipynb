{"cells":[{"cell_type":"markdown","metadata":{"id":"qMZrbVLN22KT"},"source":["# Run experiment multi-output regression\n","\n","\n","---\n","\n","\n","The aim of this script is to run the experiment in order to train a multioutput regression neural network to predict the size of cell parameters of different crystal structures. Users have to load the crystal structure dataset, read it, select which are the features to be used to train the model, as well as, which are the response variables. \n","\n","After that, dataset will be splitted in 80% for training and 20% for test dataset. Training dataset will be splitted again in 80% for training and 20% for validation, during the hyperparameters optimization. After that, the best hyperparameters will be used to fit agai the neural network on the full training dataset."]},{"cell_type":"markdown","metadata":{"id":"JOohr7VS5Sop"},"source":["## Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NEPe4FyzlDkX"},"outputs":[],"source":["!pip install keras-tuner --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UbOdIUj45VdH"},"outputs":[],"source":["import os\n","\n","# Data Manipulation\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import reciprocal\n","\n","# Data Visualization\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","# TensorFlow / Keras \n","from tensorflow import keras\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.utils import plot_model\n","from tensorflow.keras.optimizers import Adam, SGD, Adagrad, RMSprop\n","from tensorflow.keras.optimizers.schedules import ExponentialDecay, PiecewiseConstantDecay, PolynomialDecay\n","from tensorflow.keras.metrics import RootMeanSquaredError, MeanAbsolutePercentageError\n","import keras_tuner as kt\n","import tensorflow as tf\n","\n","# Scikit-learn\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score\n","from sklearn import model_selection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IsY5B1yb2lkV"},"outputs":[],"source":["import warnings \n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"ndMdVy715c-T"},"source":["## Load the Data\n","\n","\n","---\n","\n","\n","User must enter the name of the path in which dataset is stored. After that, we will check if the directory exists and if it is empty or not."]},{"cell_type":"markdown","metadata":{"id":"mgSB0kosJBP_"},"source":["### *Check directory and files*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6TdWOLS5cYb"},"outputs":[],"source":["def read_dataset ():\n","  dataset = ''\n","  path_name = input('Enter the path name for dataset: ')\n","  path_name = '/content/' + path_name\n","\n","  if not os.path.exists(path_name):\n","      print('Error! Invalid path selected.')\n","  else:\n","      print(path_name + ' is a valid path.')\n","\n","      if not os.listdir(path_name):\n","        print(\"Warning! Empty directory.\")\n","      else:\n","        file_name = input('Enter the file name for dataset: ')\n","        dataset = pd.read_csv(path_name + '/' + file_name + '.csv', sep = ';', index_col = 'ID_Observations' )\n","  return dataset"]},{"cell_type":"markdown","metadata":{"id":"NgixnYkv5vPQ"},"source":["### *Load the dataset*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXVkwuHJ5yR7"},"outputs":[],"source":["y_coord_dataset = read_dataset()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e8hd6CI8558r"},"outputs":[],"source":["print('y_coord_dataset shape: {}'.format(y_coord_dataset.shape))\n","print('\\n data types: \\n{}'.format(y_coord_dataset.dtypes))\n","print('\\ny_coord_dataset content: \\n')\n","y_coord_dataset"]},{"cell_type":"markdown","metadata":{"id":"u_XBPrEl6Tzp"},"source":["## Insert details on dataset features\n","\n","\n","---\n","\n","\n","Users have to specify which variables will be used as features and which will be response features."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"W-6FLTa16jzc"},"outputs":[],"source":["def insert_dataset_structure_details(final_dataset):\n","    print('============================================================================')\n","    print('Insert dataset structure details');\n","\n","    while 1:\n","        print('----------------------------------------------------------------------------')\n","        use_volume = input('\\n1) Do you want to use volume as a feature in the experiment? [Y|N]: ');\n","        if use_volume.lower() == 'y':\n","          use_volume = True;\n","          break;\n","        elif use_volume.lower() == 'n':\n","          use_volume = False;\n","          break;\n","\n","    while 1:\n","        print('----------------------------------------------------------------------------')\n","        use_total_n_peaks = input('\\n2) Do you want to use total_n_peaks as a feature in the experiment? [Y|N]: ');\n","        if use_total_n_peaks.lower() == 'y':\n","          use_total_n_peaks = True;\n","          break;\n","        elif use_total_n_peaks.lower() == 'n':\n","          use_total_n_peaks = False;\n","          break;\n","\n","    while 1:\n","        print('----------------------------------------------------------------------------')\n","        use_max_peaks = input('\\n3) Do you want to use max_peaks as a feature in the experiment? [Y|N]: ');\n","        if use_max_peaks.lower() == 'y':\n","          use_max_peaks = True;\n","          break;\n","        elif use_max_peaks.lower() == 'n':\n","          use_max_peaks = False;\n","          break;\n","   \n","    while 1:\n","        print('----------------------------------------------------------------------------')\n","        a_is_response = input('\\n4) \"a\" is a response feature ? [Y|N]: ');\n","        if a_is_response.lower() == 'y':\n","          a_is_response = True;\n","          break;\n","        elif a_is_response.lower() == 'n':\n","          a_is_response = False;\n","          break;\n","\n","    while 1:\n","        print('----------------------------------------------------------------------------')\n","        b_is_response = input('\\n5) \"b\" is a response feature ? [Y|N]: ');\n","        if b_is_response.lower() == 'y':\n","          b_is_response = True;\n","          break;\n","        elif b_is_response.lower() == 'n':\n","          b_is_response = False;\n","          break;\n","\n","    while 1:\n","        print('----------------------------------------------------------------------------')\n","        c_is_response = input('\\n6) \"c\" is a response feature ? [Y|N]: ');\n","        if c_is_response.lower() == 'y':\n","          c_is_response = True;\n","          break;\n","        elif c_is_response.lower() == 'n':\n","          c_is_response = False;\n","          break;\n","\n","    print('============================================================================')\n","\n","    response_features_name = []\n","\n","    if not use_total_n_peaks:\n","      final_dataset.drop('Total_n_peaks', axis=1, inplace=True)\n","    \n","    if not use_max_peaks:\n","      final_dataset.drop('Max_peaks_position', axis=1, inplace=True)\n","\n","    if not use_volume:\n","      final_dataset.drop('Volume', axis=1, inplace=True)\n","    \n","    if not a_is_response:\n","      final_dataset.drop('a', axis=1, inplace=True)\n","    else:\n","      response_features_name.append('a')\n","    \n","    if not b_is_response:\n","      final_dataset.drop('b', axis=1, inplace=True)\n","    else:\n","      response_features_name.append('b')\n","    \n","    if not c_is_response:\n","      final_dataset.drop('c', axis=1, inplace=True)  \n","    else:\n","      response_features_name.append('c')\n","\n","    final_dataset.drop('Crystal_Structure_Type', axis=1, inplace=True)\n","\n","    return final_dataset, response_features_name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbFBEeYD9vvU"},"outputs":[],"source":["final_dataset_for_experiment, response_features_name = insert_dataset_structure_details(y_coord_dataset.copy())\n","print('The response features for this crystal structure are: {}'.format(response_features_name))\n","print('The final dataset with features is:\\n')\n","final_dataset_for_experiment"]},{"cell_type":"markdown","metadata":{"id":"d74ocequF0h_"},"source":["## Splitting data for regression task"]},{"cell_type":"markdown","metadata":{"id":"RSXGeAoeFIbW"},"source":["### *Separating Input Features and Output Features*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUW84c9ZD84C"},"outputs":[],"source":["x = final_dataset_for_experiment[final_dataset_for_experiment.columns.difference(response_features_name)]\n","print('The dataset has the following number of input features: {:d}'.format(x.shape[1]))\n","print('Input features to be used for training are: ')\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5R9oCFUGQe8"},"outputs":[],"source":["y = final_dataset_for_experiment[response_features_name]\n","print('The dataset has the following number of output features: {:d}'.format(y.shape[1]))\n","print('Output features are:')\n","y"]},{"cell_type":"markdown","metadata":{"id":"M6D5Y_vll4o_"},"source":["### *Splitting the dataset into training set, validation set and test set*\n","We use 80% of the original dataset as training dataset and the remaining 20% to test the model.\n","The training set (80%) is splitted again in training dataset (80%) and validation dataset (20%)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNiKQ7fql4R4"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, shuffle= True )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fL2xqBy4CQJ"},"outputs":[],"source":["x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, shuffle= True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yZlw3KbIntcR"},"outputs":[],"source":["print('Training-Validation-Test dataset shape')\n","print('------------------------------------------------------------------------')\n","print('- The training set has the following number of observations: {:d}'.format(y_train.shape[0])) \n","print('- The validation set has the following number of observations: {:d}'.format(y_val.shape[0]))\n","print('- The test set has the following number of observations: {:d}'.format(y_test.shape[0]))"]},{"cell_type":"markdown","metadata":{"id":"_BILdTIathON"},"source":["#### *Boxplot distribution of response in training, validation and test set*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ashm1QXBm-F0"},"outputs":[],"source":["fig, axes = plt.subplots(nrows=1, ncols=3)\n","y_train.boxplot(column=y_train.columns.tolist(), ax=axes[0])\n","axes[0].set_title('Training')\n","\n","y_val.boxplot(column=y_val.columns.tolist(), ax=axes[1])\n","axes[1].set_title('Validation')\n","\n","y_test.boxplot(column=y_test.columns.tolist(), ax=axes[2])\n","axes[2].set_title('Test')\n","\n","fig.tight_layout(h_pad=1)\n","fig.suptitle('Output distribution')\n","plt.subplots_adjust(top=0.85)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-HL4W2kemvY"},"outputs":[],"source":["fig.savefig('hexagonal_volume_boxplot_output_distribution.png')"]},{"cell_type":"markdown","metadata":{"id":"C_PHD3gFz-EA"},"source":["## Training multi-output regression neural network model\n","\n","---\n","\n","After splitting the data into training, validation and testing set, it's time to train our neural network model. "]},{"cell_type":"markdown","metadata":{"id":"M67YnAZp0qmH"},"source":["### *Define the neural network architectures and search space for hyperparameters*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1yqGb1IGMhf"},"outputs":[],"source":["def build_model(hp):\n","  model = Sequential(name='Multi_Output_Neural_Network')\n","\n","  model.add(Dense(units=hp.Int('units_input_layer', min_value=32, max_value=512, step=32),\n","                  input_shape = (x_train.shape[1],),\n","                  name='Input_Layer',\n","                  #kernel_initializer=hp.Choice(f'init_weights_input_layer', ['he_normal','he_uniform','glorot_uniform','glorot_normal']), \n","                  kernel_initializer='he_normal',\n","                  bias_initializer='zeros'\n","                  )\n","  )\n","                  \n","  model.add(Dropout(rate=hp.Float('dropout_rate_input_layer', \n","                                     min_value=0.0, max_value=0.9, step=0.1), \n","                    name=f'Dropout_Input_Layer'))\n","  \n","  # Optimize the number of hidden layers\n","  for i in range(hp.Int('num_hidden_layers', 1, 6)):\n","    model.add(Dense(units=hp.Int(f'units_hidden_layer_{i}', min_value=32, max_value=512, step=32),\n","                    activation=hp.Choice(f'activation_fun_hidden_layer_{i}', ['relu','tanh', 'softmax', 'softplus', 'selu', 'elu', 'softsign']), \n","                    name=f'Hidden_Layer_{i+1}',\n","                    #kernel_initializer=hp.Choice(f'init_weights_hidden_layer_{i}', ['he_normal','he_uniform','glorot_uniform','glorot_normal']), \n","                    kernel_initializer='he_normal',\n","                    bias_initializer='zeros'\n","                    ))\n","    \n","    model.add(Dropout(rate=hp.Float(f'dropout_rate_hidden_layer_{i}', \n","                                     min_value=0.0, max_value=0.9, step=0.1), name=f'Dropout_Layer_{i+1}'))\n","    \n","  # Add an output layer with n neurons, one for each output\n","  model.add(Dense(y_train.shape[1], activation='linear', name='Output_Layer'))\n","\n","  # Define the optimizer learning rate as a hyperparameter.\n","  lr = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='log')\n","  #lr = ExponentialDecay(0.1, decay_steps=100000, decay_rate=0.96,staircase=True)\n","  #lr =  PolynomialDecay(0.1,10000,0.01,power=0.5)\n","\n","  model.compile(\n","      optimizer=Adam(learning_rate=lr),\n","      #optimizer=Adam(learning_rate=learning_rate_schedule_dict[hp_learning_rate_schedule]),\n","      #optimizer=optimizers_dict[hp_optimizers],\n","      loss=['mean_squared_error'],\n","      metrics=[RootMeanSquaredError(), MeanAbsolutePercentageError()], \n","      )\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HvoEGrKTI7Zb"},"outputs":[],"source":["build_model(kt.HyperParameters())"]},{"cell_type":"markdown","metadata":{"id":"g4cUQgtXKsSe"},"source":["### *Define settings for hyperparameters search*\n","\n","\n","*   **MAX_TRIALS** - Number of hyperparameter combinations that will be tested by the tuner\n","*   **EXECUTION_PER_TRIAL** - The number of models that should be built and fit for each trial. Different trials have different hyperparameter values. The executions within the same trial have the same hyperparameter values. The purpose of having multiple executions per trial is to reduce results variance and therefore be able to more accurately assess the performance of a model. \n","*  **MAX_EPOCHS** - Maximum number of epochs to train one model\n","*  **MINI_BATCH** - Size of subset of the dataset used to take another step in the learning process.Instead of waiting for the model to compute the whole dataset, we’re able to update its parameters more frequently. This reduces the risk of getting stuck at a local minimum, since different batches will be considered at each iteration, granting a robust convergence.\n","*  **PATIENCE** - Number of epochs with no improvement after which training will be stopped.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iI_BljBXKzeE"},"outputs":[],"source":["MAX_TRIALS = 50\n","EXECUTION_PER_TRIAL = 3\n","MAX_EPOCHS = 250\n","MINI_BATCH_SIZE = 32\n","PATIENCE = 5"]},{"cell_type":"markdown","metadata":{"id":"tbOb3UnXFCdB"},"source":["### *Start the search*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Hvexa5iJaih"},"outputs":[],"source":["tuner = kt.BayesianOptimization(\n","    hypermodel=build_model,\n","    objective=kt.Objective('val_loss', direction='min'),\n","    max_trials=MAX_TRIALS,\n","    executions_per_trial=EXECUTION_PER_TRIAL,\n","    distribution_strategy=tf.distribute.MirroredStrategy(),\n","    directory='neural-network-optimization-hexagonal',\n","    project_name='hexagonal-volume-optimization-neural-network',\n","    overwrite=True,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jeDsMrPr9_A"},"outputs":[],"source":["tuner.search_space_summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8kti9ysCsWUy"},"outputs":[],"source":["tuner.search(x_train.to_numpy(),\n","             y_train.to_numpy(), \n","             epochs=MAX_EPOCHS,\n","             batch_size=MINI_BATCH_SIZE,\n","             validation_data=(x_val.to_numpy(), y_val.to_numpy()),\n","             callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, verbose=1, restore_best_weights = True),\n","                        keras.callbacks.TensorBoard('neural-network-optimization-hexagonal/hexagonal-volume-optimization-neural-network_logs')\n","                      ],\n","             )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"TbyQD6ha6Idh"},"outputs":[],"source":["%reload_ext tensorboard\n","%tensorboard --logdir neural-network-optimization-hexagonal/hexagonal-volume-optimization-neural-network_logs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jFBZfbUAuuwP"},"outputs":[],"source":["tuner.results_summary()"]},{"cell_type":"markdown","metadata":{"id":"mg9ecGjXFZHC"},"source":["### *Retrain the model with the best hyperparameters*\n","After the hyperparameters tuning, we take the best hyperparameters to train the model using the all training dataset (*x_train + x_val*)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1et5mep4iYkZ"},"outputs":[],"source":["x_train = pd.concat([x_train, x_val], axis=0)\n","y_train = pd.concat([y_train, y_val], axis=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rpwukeut8Q_0"},"outputs":[],"source":["print('The hyperparameter search is complete.\\nThe optimal hyperparameters are: \\n')\n","tuner.get_best_hyperparameters()[0].values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z6zMXut908nH"},"outputs":[],"source":["best_model = tuner.hypermodel.build(tuner.get_best_hyperparameters()[0])\n","training_history = best_model.fit(x_train.to_numpy(), \n","                                  y_train.to_numpy(), \n","                                  epochs=MAX_EPOCHS,\n","                                  batch_size=MINI_BATCH_SIZE, \n","                                  )"]},{"cell_type":"markdown","metadata":{"id":"efxeRKhk6PuN"},"source":["### *Show trained neural network architecture*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nhnk1DHYF2-s"},"outputs":[],"source":["best_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AMrQc9NsEH-H"},"outputs":[],"source":["plot_model(best_model)"]},{"cell_type":"markdown","metadata":{"id":"PRbMvuFkF6wy"},"source":["## Predict and evaluate the trained neural network on test set"]},{"cell_type":"markdown","metadata":{"id":"CGLXZCTw45Vz"},"source":["### *Predict on test set*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5BlynLJ1Kk9p"},"outputs":[],"source":["y_test_pred = best_model.predict(x_test.to_numpy())\n","y_test_pred"]},{"cell_type":"markdown","metadata":{"id":"kHFOtFx5oQ_5"},"source":["### *Save predicted values*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wvttNJMXz5FK"},"outputs":[],"source":["y_features_name = y_test.columns.tolist()\n","features_col_name = {}\n","\n","features_col_name = {y_features_name[i] + '_pred' for i in range(0, len(y_features_name))}\n","\n","summary_prediction = y_test.join(pd.DataFrame(y_test_pred, index=y_test.index, columns=features_col_name))\n","summary_prediction = summary_prediction.reindex(sorted(summary_prediction.columns), axis=1)\n","summary_prediction.to_csv('hexagonal_volume_summary_prediction_test.csv', sep=';', index=True, header=True)\n","summary_prediction"]},{"cell_type":"markdown","metadata":{"id":"BLmakkTy49KV"},"source":["### *Evaluate the trained model on test set*"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Bu9UQdusGhyv"},"outputs":[],"source":["r2_result=[]\n","corr_coeff_result=[]\n","for i in range(0,summary_prediction.shape[1],2):\n","  r2_result.append(r2_score(summary_prediction.iloc[i], summary_prediction.iloc[i+1]))\n","  corr_coeff_result.append(summary_prediction.iloc[:,[i,i+1]].corr().iloc[0,1])\n","\n","[mse_test, rmse_test, mape_test] = best_model.evaluate(x_test.to_numpy(),y_test.to_numpy())\n","print('- r2_score for output {}: {}'.format(y_features_name,r2_result))\n","print('- corr_coeff for output {}: {}'.format(y_features_name,corr_coeff_result))"]},{"cell_type":"markdown","metadata":{"id":"_RVfYhuDzOrL"},"source":["##### *Create PWB Table*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ygp3UsK7mi_o"},"outputs":[],"source":["def compute_pwbX(df, threshold, output_name):\n","  df_inrange = pd.DataFrame(index=df.index, columns=output_name)\n","\n","  j=0\n","  for i in range(0, df.shape[1],2):\n","    obs = df.iloc[:,0]\n","    pred = df.iloc[:,1]\n","    min_bound=obs-(obs*threshold/100)\n","    max_bound=obs+(obs*threshold/100)\n","    df_inrange.iloc[:,j] = pred.between(min_bound,max_bound)\n","    j+=1\n","\n","  return (df_inrange.all(axis='columns').values.sum())*100/df.shape[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kw1NQbcRjOlk"},"outputs":[],"source":["pwbX = [1, 5, 10, 20]\n","pwbXIndex = ['PWB' + str(pwbX[i]) for i in range(0, len(pwbX))]\n","pwbXIndex\n","\n","pwbTable = pd.DataFrame(index=pwbXIndex, columns=['% samples'])\n","pwbTable\n","\n","for i in range(0, len(pwbX)):\n","  pwbTable.iloc[i,0] = compute_pwbX(summary_prediction, pwbX[i],y_features_name)\n","\n","pwbTable"]},{"cell_type":"markdown","metadata":{"id":"9-SxpJzMt9L_"},"source":["### *Save trained model*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO7LDTOst__A"},"outputs":[],"source":["best_model.save('hexagonal_volume_neural_network.h5')"]},{"cell_type":"markdown","metadata":{"id":"LfJ6aW0bRK9V"},"source":["## Define function to plot multi-output regression results"]},{"cell_type":"markdown","metadata":{"id":"7QUiaSOgRR82"},"source":["### *Perfect fit plot*\n","*Perfect fit plot* will display a straight black line meaning real observations value are equals to predicted values, and a blue points which will represent the observations. We will plot real observations value again predicted values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_MRSdU0RW88"},"outputs":[],"source":["def plot_perfect_fit(df, output_names, title_fig):\n","\n","  n_output = df.shape[1]\n","  \n","  fig = make_subplots(\n","      rows=1, cols= round(n_output/2),\n","      subplot_titles=['<b>' + name_output + '</b>' for name_output in output_names],\n","  )\n","\n","  max_lim_axis = round(max(df.max(axis=0))+1)\n","\n","  min_lim_axis = round(min(df.min(axis=0)))\n","  if min_lim_axis > 0:\n","    min_lim_axis = 0\n","  \n","  fit_point = np.linspace(-6, max_lim_axis, max_lim_axis)\n","\n","  j = 1\n","\n","  for i in range(0,n_output,2):\n","    obs = df.iloc[:, i].to_numpy()\n","    pred = df.iloc[:, i+1].to_numpy()\n","\n","    fig.add_trace(go.Scatter(x=obs, y=pred, mode='markers', marker_color='#1F77B4', \n","                             marker_size=8, name='Observations'), row=1, col=j)\n","    \n","    fig.add_trace(go.Scatter(x=fit_point, y=fit_point, mode='lines', \n","                             line={'color': 'black'}, name='Perfect prediction'),\n","                   row=1, col=j)\n","    \n","    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', \n","                      showline=True, linewidth=2, linecolor='black', mirror=True,\n","                      title_text='True response', range=[min_lim_axis, max_lim_axis])\n","    \n","    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', \n","                      showline=True, linewidth=2, linecolor='black', mirror=True,\n","                      title_text='Predicted response', range=[min_lim_axis, max_lim_axis])\n","    if j > 1:\n","      fig.update_traces(row=1, col=j, showlegend=False)\n","    \n","    fig.layout.annotations[j-1].update(font=dict(size=20))\n","\n","    j+=1\n","\n","  fig.update_layout(\n","      template='simple_white',\n","      width=1780,\n","      height=650,\n","      title_text='<b>'+title_fig+'</b>',\n","      title_x=0.5, \n","      font=dict(size=16), \n","      legend_tracegroupgap=360,\n","      legend=dict(\n","          font=dict(size=16),\n","          bordercolor=\"Black\",\n","          borderwidth=2\n","          )\n","  )\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"brjLa_i4MjeE"},"source":["### *Residual bar plot*\n","*Residual bar plot* will display blue points which represent the real observations, the yellow points which are predicted observations and the red bar which are the residual values between real and predicted values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"okGKCAhsienR"},"outputs":[],"source":["def plot_residual_bar(df, output_names, title_fig):\n","\n","  n_observations = df.shape[0]\n","  n_output = df.shape[1]\n","  \n","  fig = make_subplots(\n","      rows=1, cols= round(n_output/2),\n","      subplot_titles=['<b>' + name_output + '</b>' for name_output in output_names],\n","  )\n","\n","  max_lim_axis = round(max(df.max(axis=0))+1)\n","\n","  min_lim_axis = round(min(df.min(axis=0)))\n","  if min_lim_axis > 0:\n","    min_lim_axis = 0\n","  \n","  index_row = np.linspace(-6, n_observations, n_observations)\n","\n","  j = 1\n","  show_error_legend_bar = True\n","\n","  for i in range(0,n_output,2):\n","    param_df = df.iloc[:, [i, i+1]]\n","    param_df = param_df.sort_values(by=param_df.columns[0])\n","    \n","    obs = param_df.iloc[:, 0].to_numpy()\n","    pred = param_df.iloc[:, 1].to_numpy()\n","\n","    for k in range(1, n_observations):\n","      fig.add_trace(go.Scatter(x=[k,k], y=[obs[k], pred[k]], name=\"Error\", \n","                                mode='lines', line={'color': '#D62728', 'width':1},\n","                               showlegend=show_error_legend_bar), row=1, col=j)\n","      if show_error_legend_bar:\n","        show_error_legend_bar=False\n","\n","    fig.add_trace(go.Scatter(x=index_row, y=obs, mode='markers', marker_color='#1F77B4', \n","                             marker_size=8, name='True'), row=1, col=j)\n","    \n","    fig.add_trace(go.Scatter(x=index_row, y=pred, mode='markers', marker_color='#FF7F0E', \n","                             marker_size=8, name='Predicted'), row=1, col=j)\n","        \n","    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', \n","                      showline=True, linewidth=2, linecolor='black', mirror=True,\n","                      title_text='Record number', range=[0, n_observations])\n","    \n","    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', \n","                      showline=True, linewidth=2, linecolor='black', mirror=True,\n","                      title_text='Response', range=[min_lim_axis, max_lim_axis])\n","    if j > 1:\n","      fig.update_traces(row=1, col=j, showlegend=False)\n","    \n","    fig.layout.annotations[j-1].update(font=dict(size=20))\n","\n","    j+=1\n","\n","  fig.update_layout(\n","      template='simple_white',\n","      width=1780,\n","      height=650,\n","      title_text='<b>'+title_fig+'</b>',\n","      title_x=0.5, \n","      font=dict(size=16), \n","      legend_tracegroupgap=360,\n","      legend=dict(\n","          font=dict(size=16),\n","          bordercolor=\"Black\",\n","          borderwidth=2\n","          )\n","  )\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"QHqqjs9Qu2PZ"},"source":["### *Compare observations plot*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NpaRXD4Qu2bT"},"outputs":[],"source":["def plot_compare_observations(df, output_names, title_fig):\n","\n","  n_observations = df.shape[0]\n","  n_output = df.shape[1]\n","  \n","  fig = make_subplots(\n","      rows=round(n_output/2), cols=1,\n","      subplot_titles=['<b>' + name_output + '</b>' for name_output in output_names],\n","  )\n","\n","  max_lim_axis = round(max(df.max(axis=0))+1)\n","\n","  min_lim_axis = round(min(df.min(axis=0)))\n","  if min_lim_axis > 0:\n","    min_lim_axis = 0\n","  \n","  index_row = np.linspace(0, n_observations, n_observations)\n","\n","  j = 1\n","\n","  for i in range(0,n_output,2):\n","    obs = df.iloc[:, i].to_numpy()\n","    pred = df.iloc[:, i+1].to_numpy()\n","\n","    fig.add_trace(go.Scatter(x=index_row, y=obs, \n","                             mode='lines', line={'color': '#1F77B4', 'width':1},\n","                             name='True'), row=j, col=1)\n","    \n","    fig.add_trace(go.Scatter(x=index_row, y=pred, \n","                             mode='lines', line={'color': '#FF7F0E', 'width':1}, \n","                             name='Predicted'),row=j, col=1)\n","    \n","    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', \n","                      showline=True, linewidth=2, linecolor='black', mirror=True,\n","                      title_text='Record number', range=[0, n_observations])\n","    \n","    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGrey', \n","                      showline=True, linewidth=2, linecolor='black', mirror=True,\n","                      title_text='Response', range=[min_lim_axis, max_lim_axis])\n","    if j > 1:\n","      fig.update_traces(row=j, col=1, showlegend=False)\n","    \n","    fig.layout.annotations[j-1].update(font=dict(size=20))\n","\n","    j+=1\n","\n","  fig.update_layout(\n","      template='simple_white',\n","      width=1780,\n","      height=1500,\n","      title_text='<b>'+title_fig+'</b>',\n","      title_x=0.5, \n","      font=dict(size=16), \n","      legend_tracegroupgap=360,\n","      legend=dict(\n","          font=dict(size=16),\n","          bordercolor=\"Black\",\n","          borderwidth=2\n","          )\n","  )\n","\n","  fig.show()"]},{"cell_type":"markdown","metadata":{"id":"hQH24ur55dXF"},"source":["## Plot regression results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OmiNE5IidimM"},"outputs":[],"source":["summary_prediction[['b']] = summary_prediction.loc[:, ['a']]\n","summary_prediction[['b_pred']] = summary_prediction.loc[:, ['a_pred']]\n","#summary_prediction[['c']] = summary_prediction.loc[:, ['a']]\n","#summary_prediction[['c_pred']] = summary_prediction.loc[:, ['a_pred']]\n","summary_prediction = summary_prediction.reindex(sorted(summary_prediction.columns), axis=1)\n","summary_prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YTUmUCMoRxiW"},"outputs":[],"source":["#plot_perfect_fit(summary_prediction,y_test.columns.tolist(), 'Cubic')\n","plot_perfect_fit(summary_prediction,['a','b','c'], 'Hexagonal')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8kWqwsshV3_j"},"outputs":[],"source":["#plot_residual_bar(summary_prediction,y_test.columns.tolist(), 'Cubic')\n","plot_residual_bar(summary_prediction,['a','b','c'], 'Hexagonal')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxrVocMZTpEc"},"outputs":[],"source":["#plot_compare_observations(summary_prediction,y_test.columns.tolist(), 'Cubic')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["JOohr7VS5Sop","mgSB0kosJBP_","7QUiaSOgRR82","brjLa_i4MjeE","QHqqjs9Qu2PZ"],"provenance":[],"authorship_tag":"ABX9TyOL9ojR8AozJ1fcqzKizmU+"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}