{"cells":[{"cell_type":"markdown","metadata":{"id":"pyHJOD2bHw3U"},"source":["# Merging raw data from crystal dataset\n","\n","\n","---\n","\n","\n","The aim of this script is to merge in a more confortable way the huge raw dataset available. In particular, for each crystal structure (we have 7 different crystal structure), the releated dataset will be loaded from a large-text-file. As output of this script we want a dataframe (X) which collect all the spectrum position (from 0 to 90, increasing by 0.02) and 7 different dataframe (one for each crystal structure), which collect the releated intensity with respect the X position in the spectrum. Moreover, in each Y dataset, we collect also the cell paramenters size, which will be used as target features in future task. Lastly, dataset will be saved."]},{"cell_type":"markdown","metadata":{"id":"wt3u9-XoJf7-"},"source":["## Import library"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"svVbGCt0GbiW"},"outputs":[],"source":["import os\n","from google.colab import files\n","\n","# Data Manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# Data Visualization\n","import seaborn as sns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzkSzN_mw-T6"},"outputs":[],"source":["import warnings \n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"1Dccmk-CdTti"},"source":["## Create dataframes to store dataset \n","---\n","For each dataset, script will produce two different dataframe in output as followed:\n","*   ***x_coord_dataset***, which is a dataframe of size 1-by-4501  with information about the x-coord of the spectrum. In particular, we store point in the following interval [0,90] increasing by 0.02;\n","*   ***y_coord_dataset***, which is a datframe of size num_obs-by-4502 storing information about the recordered intensity, releated to each point stored in x-coord-dataset, with also id-observation\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nqoK0aUBfRud"},"source":["### *Create dataframe to store the x-coordinates of the spectrum for each observation*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BVuw7PBdTIU"},"outputs":[],"source":["x_coord_dataset = pd.DataFrame(np.arange(0,90.02,0.02)).T\n","x_coord_dataset.columns = [f'x_{i}' for i in range(1,len(x_coord_dataset.T)+1)]   # Rename x_coord_dataset columns name\n","print('The x_coord_dataset has the following shape: ', x_coord_dataset.shape)     # Check the shape of x_coord_dataset\n","x_coord_dataset"]},{"cell_type":"markdown","metadata":{"id":"wm-CS9rxfd6S"},"source":["### *Create a dataframe to store the y-coordinates of the spectrum for each observation*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFC4gRBvfk9G"},"outputs":[],"source":["y_coord_dataset = pd.DataFrame(columns = [f'y_{i}' for i in range(1,4502)])\n","y_coord_dataset['ID_Observations'] = ''\n","y_coord_dataset"]},{"cell_type":"markdown","metadata":{"id":"8c3sb8AffliH"},"source":["### *Create dataframe to store additional information*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bshJK0HHflw7"},"outputs":[],"source":["additional_information_dataset = pd.DataFrame(columns=['ID_Observations', 'a', \n","                                                       'b', 'c', 'alpha', 'beta',\n","                                                       'gamma', 'Volume', \n","                                                       'Crystal_Structure_Type'])\n","additional_information_dataset"]},{"cell_type":"markdown","metadata":{"id":"4916x2WWz6JG"},"source":["## Load the data\n","\n","\n","---\n","\n","\n","User must enter the name of the path in which dataset is stored. After that, we will check if the directory exists and if it is empty or not."]},{"cell_type":"markdown","source":["### *Check directory and files*"],"metadata":{"id":"_vqxnU0fTWiN"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":39},"id":"JV3_61OdtIWA","outputId":"57092a51-aaba-4f91-c0b9-268e882e2323"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-3d11bb74-eb8c-40e2-9d7b-c3d9b6d4699c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3d11bb74-eb8c-40e2-9d7b-c3d9b6d4699c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["UploadedFiles = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TBH3Q0Pp_5Wc"},"outputs":[],"source":["path_name = input('Enter the path name for dataset: ')\n","path_name = '/content/' + path_name\n","dir_list = ''\n","\n","if not os.path.exists(path_name):\n","    print('Error! Invalid path selected.')\n","else:\n","    print(path_name + ' is a valid path.')\n","\n","    if not os.listdir(path_name):\n","      print(\"Warning! Empty directory.\")\n","    else:\n","      dir_list = os.listdir(path_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"toC_Hnqczioo"},"outputs":[],"source":["dir_list"]},{"cell_type":"markdown","metadata":{"id":"WMJUMe4bHkLR"},"source":["### *Load the dataset using chunk*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QerPcf7Iu-ym"},"outputs":[],"source":["chunksize = 150\n","k = 1\n","for file in dir_list:\n","  print('============================================================================')\n","  print('WORKING ON FILE NUMBER: {:d}'.format(k))\n","\n","  i = 1\n","\n","  with pd.read_csv(path_name + '/' + file, sep='|', header=None, index_col = None, \n","                  names=[\"Spectrum_Data\", \"Cell_Parameters\", \"Volume\", \n","                          \"ID_Observations\", \"Crystal_Structure_Type\"], \n","                  chunksize=chunksize, usecols = [0,1,2,3,4]) as reader:\n","                  for chunk in reader:\n","                    print('----------------------------------------------------------------------------')\n","                    print('Working of chunk number: {:d}'.format(i))\n","                    print('----------------------------------------------------------------------------')\n","                    print(\"DataType: {} \\nShape: {} \\nMemory: {}\".format(type(chunk),\n","                                                chunk.shape, \n","                                                chunk.memory_usage().sum()))\n","                    \n","                    # Retrive field for additional_information_dataset dataframe\n","                    chunk[['a', 'b', 'c', 'alpha', 'beta', 'gamma',]] = chunk.Cell_Parameters.str.split(' ', expand=True)\n","                    chunk.pop('Cell_Parameters')\n","                    additional_information_single_chunk = chunk[['ID_Observations', \n","                                                                  'a', 'b', 'c', \n","                                                                  'alpha', 'beta',\n","                                                                  'gamma', 'Volume',\n","                                                                  'Crystal_Structure_Type']]\n","                    additional_information_dataset = pd.concat([additional_information_dataset, \n","                                                                additional_information_single_chunk], ignore_index= True)\n","                    \n","                    # Retrive and split specrtum field\n","                    xy_chunk = chunk['Spectrum_Data'].str.split(' ', expand=True)\n","                    x_y_chunk = pd.DataFrame()\n","\n","                    j = 1\n","                    for col in xy_chunk.columns:\n","                      x_y_chunk[[f'x_{j}', f'y_{j}']] = xy_chunk[col].str.split(';', expand = True)\n","                      j = j + 1\n","\n","                    # Drop column with name starting with 'x_'\n","                    x_y_chunk = x_y_chunk.loc[:, ~x_y_chunk.columns.str.contains('^x_')]\n","                                        \n","                    x_y_chunk['ID_Observations'] = chunk.ID_Observations\n","\n","                    y_coord_dataset = pd.concat([y_coord_dataset, x_y_chunk], ignore_index= True)\n","                    \n","                    i = i + 1\n","  k = k + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hnquzXhUCAeW"},"outputs":[],"source":["print('x_coord_dataset shape: {}'.format(x_coord_dataset.shape))\n","print('x_coord_dataset data types: \\n{}'.format(x_coord_dataset.dtypes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8JJcwoQNuxKC"},"outputs":[],"source":["print('y_coord_dataset shape: {}'.format(y_coord_dataset.shape))\n","print('y_coord_dataset data types: \\n{}'.format(y_coord_dataset.dtypes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ImGLUywV3a52"},"outputs":[],"source":["print('additional_information_dataset shape: {}'.format(additional_information_dataset.shape))\n","print('additional_information_dataset data types: \\n{}'.format(additional_information_dataset.dtypes))"]},{"cell_type":"markdown","metadata":{"id":"rK4h-wgHCJ3f"},"source":["## Basic checks"]},{"cell_type":"markdown","metadata":{"id":"-m2fya6IFHSf"},"source":["### *Casting of numeric field from object to float*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HJMPPMqcFj8p"},"outputs":[],"source":["index_name = y_coord_dataset.select_dtypes(include='object').columns\n","index_name = index_name.drop('ID_Observations')\n","y_coord_dataset[index_name] = y_coord_dataset[index_name].astype(\"float\")\n","print('y_coord_dataset data types after casting operation: \\n{}'.format(y_coord_dataset.dtypes))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4u7SXOPuoTN"},"outputs":[],"source":["index_name = additional_information_dataset.select_dtypes(include='object').columns\n","index_name = index_name.drop('Crystal_Structure_Type')\n","index_name = index_name.drop('ID_Observations')\n","additional_information_dataset[index_name] = additional_information_dataset[index_name].astype(\"float\")\n","print('additional_information_dataset data types after casting operation: \\n{}'.format(additional_information_dataset.dtypes))"]},{"cell_type":"markdown","metadata":{"id":"LI-A7lzGI5kO"},"source":["### *Merging additional_information_dataset and y_coord_dataset on ID_Observations*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zm6rMXqLGhpV"},"outputs":[],"source":["y_coord_dataset = y_coord_dataset.merge(additional_information_dataset, on = 'ID_Observations')\n","print('y_coord_dataset shape: {}'.format(y_coord_dataset.shape))\n","print('y_coord_dataset data types: \\n{}'.format(y_coord_dataset.dtypes))"]},{"cell_type":"markdown","metadata":{"id":"gc2hiJQ0KY6N"},"source":["### *Check for ***null*** field*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WswbRfYGht1"},"outputs":[],"source":["print('Null field in y_coord_dataset: \\n{}'.format(y_coord_dataset.isnull().any()))\n","sns.heatmap(y_coord_dataset.isnull(), cbar=False)"]},{"cell_type":"markdown","metadata":{"id":"mLbCiwTrVp5I"},"source":["### *Check for NaN rows and drop it*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QhpOjCf4VqOD"},"outputs":[],"source":["n_samples = y_coord_dataset.shape[0]\n","y_coord_dataset = y_coord_dataset.dropna()   \n","print('Number of NaN rows dropped: {}/{} ({:.2f}%)'.format(n_samples - y_coord_dataset.shape[0], n_samples, (n_samples - y_coord_dataset.shape[0]) / n_samples * 100))"]},{"cell_type":"markdown","metadata":{"id":"CQUkwEcON_33"},"source":["### *Check for duplicate rows and drop, if exists*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_EZsG5FEGhz9"},"outputs":[],"source":["duplicated_rows = y_coord_dataset[y_coord_dataset.duplicated()]\n","n_samples = y_coord_dataset.shape[0]  \n","duplicates = n_samples - y_coord_dataset.shape[0]\n","\n","if not duplicated_rows.empty:\n","  y_coord_dataset.drop_duplicates(ignore_index=True, inplace=True)\n","  print('Number of canceled duplicates: {}/{} ({:.2f}%)'.format(duplicates, n_samples, duplicates / n_samples * 100))\n","else:\n","  print('Number of canceled duplicates: {}/{} ({:.2f}%)'.format(duplicates, n_samples, duplicates / n_samples * 100))"]},{"cell_type":"markdown","metadata":{"id":"DEtPlW9HkBXi"},"source":["## Show dataset content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2D7kNfz0kGvx"},"outputs":[],"source":["print('x_coord_dataset content: ')\n","x_coord_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VBwUKThXkG9W"},"outputs":[],"source":["print('y_coord_dataset content: ')\n","y_coord_dataset"]},{"cell_type":"markdown","metadata":{"id":"Q15HrR3le2eT"},"source":["## Saving the new dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zXKJKZE0Gh6A"},"outputs":[],"source":["def save_dataset(dataset):\n","  save_data = input('Do you want to save x_coord_dataset [y|n]: ')\n","  if save_data.lower() == 'y':\n","    path_dataset = input('Enter path in which to store dataset: ')\n","    name_dataset = input('Enter dataset name you want to save: ')\n","    valid_dataset_path = '/content/' + path_dataset\n","\n","    if os.path.exists(valid_dataset_path):\n","      complete_path = valid_dataset_path + '/' + name_dataset + '.csv'\n","      dataset.to_csv(complete_path, sep=';', index=False, header=True)\n","      print('Dataset stored in : ',complete_path)\n","    else:\n","      print('Error! Invalid name of dataset or Not uploaded dataset has been requested.\\n'\n","            +'Please, enter a valid dataset name to continue.')\n","  elif save_data.lower() == 'n':\n","    print('Dataset will NOT be saved!')"]},{"cell_type":"markdown","metadata":{"id":"FiB1Rv3ijajJ"},"source":["### *Saving x_coord_dataset*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mkVMD16TiVHi"},"outputs":[],"source":["save_dataset(x_coord_dataset)"]},{"cell_type":"markdown","metadata":{"id":"iJkSYlDxjgzk"},"source":["### *Saving y_coord_dataset*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0He7L3bHjnU0"},"outputs":[],"source":["save_dataset(y_coord_dataset)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"authorship_tag":"ABX9TyOe68Kq0o6Xsk0sb+8Kbnro"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}